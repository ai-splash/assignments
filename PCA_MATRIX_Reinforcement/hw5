Instruction
This HW includes both theory and implementation problems. Please note,
â€¢ Your code must work with Python 3.7+
â€¢ You need to submit a report in PDF including all written deliverable, and all implementation
codes so one could regenerate your results.
â€¢ For programming part of PCA problem, you can import any module you would like from
scikit-learn or other external libraries to minimize the amount of implementation required to
get the coding problem done. Submit your code in Problem.py.
PCA

Problem 1. [20 points] As we discussed in the lecture, the PCA algorithm is sensitive to scaling
and pre-processing of data. In this problem, we explore few data pre-processing operations on data
matrices.
1. Explain what does each of the following data processing operation mean, why it is important,
and how you apply to a data matrix X âˆˆ R nÃ—d , n samples each with d features 1 (if it helps,
use mathematical equations to show the calculations).
(a) Centering or mean removal
(b) Scaling of a feature to a range [a, b]
(c) Standardization
(d) Normalization (between 0 and 1)
2. Apply the above operations to the following data matrix with 5 samples and two features
independently and show the processed data matrix. For scaling pick a = 0 and b = 1.
ï£®
ï£¹
1 2
ï£¯ âˆ’1 1 ï£º
ï£¯
ï£º
ï£º
0
1
X = ï£¯
ï£¯
ï£º
ï£° 2 4 ï£»
3 1
1
For an implementation of these operation in scikit-learn please see this.
1Problem 2. [20 points] In this assignment, you will explore PCA as a technique for discerning
whether low-dimensional structure exists in a set of data and for finding good representations of
the data in that subspace. To this end, you will do PCA on a Iris dataset which can be loaded in
scikit-learn using following commands:
from sklearn.datasets import load_iris iris = load_iris()
X = iris.data
y = iris.target

1. Carry out a principal component analysis on the entire raw dataset (4-dimensional instances)
for k = 1, 2, 3, 4 components. How much of variance in data can be explained by the first
principal component? How does the fraction of variation explained in data vary as k varies?


2. Apply the preprocessing operations from Problem 1 on raw data set and repeat part (1) on
processed data. Explain any differences you observe compared to part (1) and justify.


3. Project the raw four dimensional data down to a two dimensional subspace generated by first
two top principle components (PCs) from part (2) and produce a scatter plot of the data.
Make sure you plot each of the three classes differently (using color or different markers).
Can you see the three Iris flower clusters?


4. Either use your k-means++ implementation from previous homework or from scikit-learn to
cluster data from part (3) into three clusters. Explain your observations.
Matrix Factorization
Problem 3. [30 points] Recall the following objective for extracting latent features from a partially
observed rating matrix via matrix factorization (MF) for making recommendations, discussed in
the class:
ï£®
ï£¹
n
m
X
X
X
2
min ï£° f (U , V ) â‰¡
(r i,j âˆ’ u >
ku i k 22 + Î²
kv j k 22 ï£»
(1)
i v j ) + Î±
U ,V
i=1
(i,j)âˆˆÎ©
j=1
where
â€¢ n: number of users
â€¢ m: number of items
â€¢ r i,j : (i, j)-th element in R âˆˆ R nÃ—m input partially observed rating matrix
â€¢ Î© âŠ† [n] Ã— [m]: index of observed entries in rating matrix, where [n] denotes the sequence of
numbers {1, 2, . . . , n}.
â€¢ k: number of latent features
â€¢ U âˆˆ R nÃ—k : the (unknown) matrix of latent feature vectors for n users (the ith row u i âˆˆ R k
is the latent features for ith user)
2â€¢ V âˆˆ R mÃ—k : the (unknown) matrix of latent feature vectors for m items (the jth row v j âˆˆ R k
is the latent features for jth movie)
Please do the followings:


1. In solving Equation 1 with iterative Alternating Minimization algorithm (fixing V (t) and
taking gradient step for U (t) and vice versa), discuss what happens if U (0) and V (0) are
initialized to zero?


2. Discuss why when there is no regularization in basic MF formulated in Equation 1, i.e.,
Î± = Î² = 0, each user must have rated at least k movies, and each movie must have been
rated by at least k users (Hint: consider the closed form solution for u i and v j in notes and
argue why these conditions are necessary without regularization).


3. Computing the closed form solution in part (2) could be computational burden for large
number of users or movies. A remedy for this would be using iterative optimization algorithms
such as Stochastic Gradient Descent (SGD) where you sample movies in updating the latent
features for users and sample users in updating the latent features of movies. Derive the
(t)
(t)
updating rules for u i and v j at tth iteration of SGD. Show the detailed steps.
Reinforcement Learning

Figure 1: A MDP with states S = {s 0 , s 1 , s 2 , s 3 } and actions A = {a 0 , a 1 , a 2 }. The reward is 10
for state s 3 , 1 for state s 2 , and 0 otherwise.
Problem 4. [20 points] Consider the MDP represented by a graph in Figure 1 with discount factor
Î³ âˆˆ [0, 1). States are represented by circles. The pair on the arrows shows the action to be taken
and the transition probability from a state to another state, respectively (please note that the
representation of MDP as a graph here is slightly different from Example 3.3 in the book). Each
of the parameters p and q are in the interval [0, 1]. The reward is 10 for state s 3 , 1 for state s 2 ,
and 0 otherwise. Note that this means, from s 0 , after taking an action, it will receive 0 reward no
matter what the action is. This is similar for other states.
31. List all two deterministic policies (for each state, choose an action with probability 1) for
MDP in Figure 1 starting from the initial state s 0 and ending at state s 3 . For each policy Ï€
compute the value of each state, v Ï€ (s 0 ), v Ï€ (s 1 ), v Ï€ (s 2 ), and v Ï€ (s 3 ) for Î³ = 0 and p = q = 2 1 .


2. Show the equation representing the optimal value function for each state, i.e. v âˆ— (s 0 ), v âˆ— (s 1 ),
v âˆ— (s 2 ), and v âˆ— (s 3 ). Please note the value of states will be a function of parameters p, q, and
Î³.


3. Using p = q = 0.25 and Î³ = 0.9, compute Ï€ âˆ— and v âˆ— for all states of MDP. You can either
solve the recursion of part (2) or implement value iteration (if you intend to do the latter,
consider the approximation error  = 10 âˆ’3 as stopping criteria).


Problem 5. [10 points] As discussed in the lecture, a key difference between Sarsa and Q-learning
is that Sarsa is on-policy, while Q-learning is off-policy. Now, Suppose action selection is greedy
in Sarsa and Q-learning. Is Q-learning then exactly the same algorithm as Sarsa? Will they make
exactly the same action selections and weight updates? The pseudocode of both algorithms are
shown below.
Figure 2: The pseudocode of SARSA and Q-Learning.
4
